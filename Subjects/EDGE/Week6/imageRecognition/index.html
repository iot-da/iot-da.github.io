<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        
        <link rel="canonical" href="https://iot-da.github.io/Subjects/EDGE/Week6/imageRecognition/">
        <link rel="shortcut icon" href="../../../../img/favicon.ico">
        <title>Re-training a Model with an input Dataset - Internet of Things and Data Analytics</title>
        <link href="../../../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">

        <script src="../../../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../../../..">Internet of Things and Data Analytics</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li >
                                <a href="../../../..">Home</a>
                            </li>
                            <li >
                                <a href="../../../../General/">General info.</a>
                            </li>
                            <li >
                                <a href="../../../../News/">News</a>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Subjects <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../../IOTNA/">IOTNA</a>
</li>
                                    
<li >
    <a href="../../../MDM/">MDM</a>
</li>
                                    
<li >
    <a href="../../../SID/">SID</a>
</li>
                                    
<li >
    <a href="../../../IA/">IA</a>
</li>
                                    
<li >
    <a href="../../../NP1/">NP1</a>
</li>
                                    
<li >
    <a href="../../../NP2/">NP2</a>
</li>
                                    
<li >
    <a href="../../../SEC/">SEC</a>
</li>
                                    
<li >
    <a href="../../">EDGE</a>
</li>
                                </ul>
                            </li>
                            <li >
                                <a href="../../../../Contact/">Contact</a>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#re-training-a-model-with-an-input-dataset">Re-training a Model with an input Dataset</a></li>
            <li><a href="#re-training-on-the-catdog-dataset">Re-training on the Cat/Dog Dataset</a></li>
            <li><a href="#downloading-the-data">Downloading the Data</a></li>
            <li><a href="#re-training-resnet-18-model">Re-training ResNet-18 Model</a></li>
            <li><a href="#converting-the-model-to-onnx">Converting the Model to ONNX</a></li>
            <li><a href="#processing-images-with-tensorrt">Processing Images with TensorRT</a></li>
            <li><a href="#running-the-live-camera-program">Running the Live Camera Program</a></li>
            <li><a href="#creating-your-own-classification-datasets">Creating your own Classification Datasets</a></li>
            <li><a href="#creating-the-label-file">Creating the Label File</a></li>
            <li><a href="#assignments">Assignments</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="re-training-a-model-with-an-input-dataset">Re-training a Model with an input Dataset</h1>
<ul>
<li><strong>Note:</strong> information extracted from <a href="https://github.com/dusty-nv/jetson-inference/blob/master/docs/pytorch-cat-dog.md">NVIDIA laboratories</a> </li>
<li>In this lab you should learn:<ul>
<li>How to download a prepared dataset to retrained a model</li>
<li>Train a CNN ResNet-18 with <strong>PyTorch</strong> (it could be done in a server/cloud system)</li>
<li>Convert the model trained to ONNX format to be run on the Jetson-Nano</li>
<li>Perform an inference in the Jetson-Nano (single image, a batch of images or using the CSI Camera)</li>
<li>Create your own dataset and repeat the training/inference process</li>
</ul>
</li>
</ul>
<h2 id="re-training-on-the-catdog-dataset">Re-training on the Cat/Dog Dataset</h2>
<p>The model that we'll be re-training is a simple model that recognizes two classes:  cat or dog.</p>
<p><img src="https://github.com/dusty-nv/jetson-inference/raw/python/docs/images/pytorch-cat-dog.jpg" width="700"></p>
<p>Provided below is an 800MB data-set that includes 5000 training images, 1000 validation images, and 200 test images, each evenly split between the cat and dog classes.  The set of training images is used for transfer learning, while the validation set is used to evaluate classification accuracy during training, and the test images are to be used by us after training completes.  The network is never directly trained on the validation and test sets, only the training set.</p>
<p>The images from the data-set are made up of many different breeds of dogs and cats, including large felines like tigers and mountain lions since the amount of cat images available was a bit lower than dogs.  Some of the images also picture humans, which the detector is essentially trained to ignore as background and focus on the cat vs. dog content.</p>
<p>To get started, first make sure that you have <a href="https://github.com/dusty-nv/jetson-inference/blob/master/docs/pytorch-transfer-learning.md#installing-pytorch">PyTorch installed</a> on your Jetson, then download the dataset below and kick off the training script.  After that, we'll test the re-trained model in TensorRT on some static images and a live camera feed. </p>
<h2 id="downloading-the-data">Downloading the Data</h2>
<p>During this tutorial, we'll store the datasets on the host device under <code>jetson-inference/python/training/classification/data</code>, which is one of the directories that is automatically mounted into the container.  This way the dataset won't be lost when you shutdown the container.</p>
<pre><code class="language-bash">nano@jetson-nano:~$ cd jetson-inference/python/training/classification/data
nano@jetson-nano:~/jetson-inference/python/training/classification/data$ wget https://nvidia.box.com/shared/static/o577zd8yp3lmxf5zhm38svrbrv45am3y.gz -O cat_dog.tar.gz
nano@jetson-nano:~/jetson-inference/python/training/classification/data$ tar xvzf cat_dog.tar.gz
</code></pre>
<p>Mirrors of the dataset are available here:</p>
<ul>
<li><a href="https://drive.google.com/file/d/16E3yFvVS2DouwgIl4TPFJvMlhGpnYWKF/view?usp=sharing">https://drive.google.com/file/d/16E3yFvVS2DouwgIl4TPFJvMlhGpnYWKF/view?usp=sharing</a></li>
<li><a href="https://nvidia.box.com/s/o577zd8yp3lmxf5zhm38svrbrv45am3y">https://nvidia.box.com/s/o577zd8yp3lmxf5zhm38svrbrv45am3y</a></li>
</ul>
<h2 id="re-training-resnet-18-model">Re-training ResNet-18 Model</h2>
<p>The PyTorch training scripts are located in the repo under <a href="https://github.com/dusty-nv/jetson-inference/tree/master/python/training/classification"><code>jetson-inference/python/training/classification/</code></a>.  These scripts aren't specific to any one dataset, so we'll use the same PyTorch code with each of the example datasets from this tutorial.  By default it's set to train a ResNet-18 model, but you can change that with the <code>--arch</code> flag.</p>
<p>To launch the training, run the following commands:</p>
<pre><code class="language-bash">nano@jetson-nano:~/jetson-inference$ sudo init 3
nano@jetson-nano:~/jetson-inference$ docker/run.sh
root@jetson-nano:/jetson-inference# cd python/training/classification
root@jetson-nano:/jetson-inference/python/training/classification# python3 train.py --batch-size=2 --workers=1  --model-dir=models/cat_dog data/cat_dog
Use GPU: 0 for training
=&gt; dataset classes:  2 ['cat', 'dog']
=&gt; using pre-trained model 'resnet18'
Downloading: &quot;https://download.pytorch.org/models/resnet18-f37072fd.pth&quot; to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth
100.0%
=&gt; reshaped ResNet fully-connected layer with: Linear(in_features=512, out_features=2, bias=True)
</code></pre>
<blockquote>
<p><strong>note:</strong> if you run out of memory or your process is "killed" during training, try <a href="https://github.com/dusty-nv/jetson-inference/blob/master/docs/pytorch-transfer-learning.md#mounting-swap">Mounting SWAP</a> and <a href="https://github.com/dusty-nv/jetson-inference/blob/master/docs/pytorch-transfer-learning.md#disabling-the-desktop-gui">Disabling the Desktop GUI</a>. <br/>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; to save memory, you can also reduce the <code>--batch-size</code> (default 8) and <code>--workers</code> (default 2)</p>
</blockquote>
<p>As training begins, you should see text appear from the console like the following:</p>
<pre><code class="language-bash">Use GPU: 0 for training
=&gt; dataset classes:  2 ['cat', 'dog']
=&gt; using pre-trained model 'resnet18'
=&gt; reshaped ResNet fully-connected layer with: Linear(in_features=512, out_features=2, bias=True)
Epoch: [0][   0/2500]  Time 95.370 (95.370)  Data  4.801 ( 4.801)  Loss 6.3362e-01 (6.3362e-01)  Acc@1  50.00 ( 50.00)  Acc@5 100.00 (100.00)
Epoch: [0][  10/2500]  Time  0.225 ( 9.306)  Data  0.000 ( 0.709)  Loss 0.0000e+00 (1.6583e+01)  Acc@1 100.00 ( 50.00)  Acc@5 100.00 (100.00)
Epoch: [0][  20/2500]  Time  0.222 ( 4.981)  Data  0.000 ( 0.377)  Loss 2.5114e+02 (6.3377e+01)  Acc@1  50.00 ( 57.14)  Acc@5 100.00 (100.00)
Epoch: [0][  30/2500]  Time  0.223 ( 3.446)  Data  0.000 ( 0.260)  Loss 1.0363e+01 (5.5885e+01)  Acc@1  50.00 ( 58.06)  Acc@5 100.00 (100.00)
Epoch: [0][  40/2500]  Time  0.226 ( 2.660)  Data  0.000 ( 0.199)  Loss 2.2514e+00 (4.3716e+01)  Acc@1   0.00 ( 52.44)  Acc@5 100.00 (100.00)

</code></pre>
<p>To stop training at any time, you can press <code>Ctrl+C</code>.  You can also restart the training again later using the <code>--resume</code> and <code>--epoch-start</code> flags, so you don't need to wait for training to complete before testing out the model.  </p>
<p>Run <code>python3 train.py --help</code> for more information about each option that's available for you to use, including other networks that you can try with the <code>--arch</code> flag.</p>
<h3 id="training-metrics">Training Metrics</h3>
<p>The statistics output above during the training process correspond to the following info:</p>
<ul>
<li>Epoch:  an epoch is one complete training pass over the dataset<ul>
<li><code>Epoch: [N]</code> means you are currently on epoch 0, 1, 2, ect.</li>
<li>The default is to run for 35 epochs (you can change this with the <code>--epochs=N</code> flag)</li>
</ul>
</li>
<li><code>[N/625]</code> is the current image batch from the epoch that you are on<ul>
<li>Training images are processed in mini-batches to improve performance</li>
<li>The default batch size is 8 images, which can be set with the <code>--batch=N</code> flag</li>
<li>Multiply the numbers in brackets by the batch size (e.g. batch <code>[100/625]</code> -&gt; image <code>[800/5000]</code>)</li>
</ul>
</li>
<li>Time:  processing time of the current image batch (in seconds)</li>
<li>Data:  disk loading time of the current image batch (in seconds)</li>
<li>Loss:  the accumulated errors that the model made (expected vs. predicted)</li>
<li><code>Acc@1</code>:  the Top-1 classification accuracy over the batch<ul>
<li>Top-1, meaning that the model predicted exactly the correct class</li>
</ul>
</li>
<li><code>Acc@5</code>:  the Top-5 classification accuracy over the batch<ul>
<li>Top-5, meaning that the correct class was one of the Top 5 outputs the model predicted</li>
<li>Since this Cat/Dog example only has 2 classes (Cat and Dog), Top-5 is always 100%</li>
<li>Other datasets from the tutorial have more than 5 classes, where Top-5 is valid </li>
</ul>
</li>
</ul>
<p>You can keep an eye on these statistics during training to gauge how well the model is trained, and if you want to keep going or stop and test.  As mentioned above, you can restart training again later if you desire.</p>
<h3 id="model-accuracy">Model Accuracy</h3>
<p>On this dataset of 5000 images, training ResNet-18 takes approximately ~7-8 minutes per epoch on Jetson Nano, or around 4 hours to train the model to 35 epochs and 80% classification accuracy.  Below is a graph for analyzing the training progression of epochs versus model accuracy:</p>
<p align="center"><img src="https://github.com/dusty-nv/jetson-inference/raw/python/docs/images/pytorch-cat-dog-training.jpg" width="700"></p>

<p>At around epoch 30, the ResNet-18 model reaches 80% accuracy, and at epoch 65 it converges on 82.5% accuracy.  With additional training time, you could further improve the accuracy by increasing the size of the dataset or by trying more complex models.</p>
<p>By default the training script is set to run for 35 epochs, but if you don't wish to wait that long to test out your model, you can exit training early and proceed to the next step (optionally re-starting the training again later from where you left off).  You can also download this completed model that was trained for a full 100 epochs from here:</p>
<ul>
<li><a href="https://nvidia.box.com/s/zlvb4y43djygotpjn6azjhwu0r3j0yxc">https://nvidia.box.com/s/zlvb4y43djygotpjn6azjhwu0r3j0yxc</a></li>
</ul>
<p>Note that the models are saved under <code>jetson-inference/python/training/classification/models/cat_dog/</code>, including a checkpoint from the latest epoch and the best-performing model that has the highest classification accuracy.  This <code>classification/models</code> directory is automatically mounted into the container, so your trained models will persist after the container is shutdown.</p>
<h2 id="converting-the-model-to-onnx">Converting the Model to ONNX</h2>
<p>To run our re-trained ResNet-18 model with TensorRT for testing and realtime inference, first we need to convert the PyTorch model into <a href="https://onnx.ai/">ONNX format</a> format so that TensorRT can load it.  ONNX is an open model format that supports many of the popular ML frameworks, including PyTorch, TensorFlow, TensorRT, and others, so it simplifies transferring models between tools.</p>
<p>PyTorch comes with built-in support for exporting PyTorch models to ONNX, so run the following command to convert our Cat/Dog model with the provided <code>onnx_export.py</code> script:</p>
<pre><code class="language-bash">root@jetson-nano:/jetson-inference/python/training/classification# python3 onnx_export.py --model-dir=models/cat_dog
Namespace(input='model_best.pth.tar', model_dir='models/cat_dog', no_softmax=False, output='')
running on device cuda:0
loading checkpoint:  models/cat_dog/model_best.pth.tar
using model:  resnet18
....
exporting model to ONNX...
....
model exported to:  models/cat_dog/resnet18.onnx
....
</code></pre>
<p>This will create a model called <code>resnet18.onnx</code> under <code>jetson-inference/python/training/classification/models/cat_dog/</code></p>
<h2 id="processing-images-with-tensorrt">Processing Images with TensorRT</h2>
<p>To classify some static test images, we'll use the extended command-line parameters to <code>imagenet</code> to load our customized ResNet-18 model that we re-trained above.  To run these commands, the working directory of your terminal should still be located in:  <code>jetson-inference/python/training/classification/</code></p>
<pre><code class="language-bash">root@jetson-nano:/jetson-inference/build/aarch64/bin# NET=/jetson-inference/python/training/classification/models/cat_dog
root@jetson-nano:/jetson-inference/build/aarch64/bin# DATASET=/jetson-inference/python/training/classification/data/cat_dog

# C++
root@jetson-nano:/jetson-inference/build/aarch64/bin# imagenet --model=$NET/resnet18.onnx --input_blob=input_0 --output_blob=output_0 --labels=$DATASET/labels.txt $DATASET/test/cat/01.jpg images/test/my_cat.jpg

# Python
root@jetson-nano:/jetson-inference/build/aarch64/bin# imagenet.py --model=$NET/resnet18.onnx --input_blob=input_0 --output_blob=output_0 --labels=$DATASET/labels.txt $DATASET/test/cat/01.jpg images/test/my_cat.jpg
</code></pre>
<p><img src="https://github.com/dusty-nv/jetson-inference/raw/python/docs/images/pytorch-cat.jpg"></p>
<h3 id="processing-all-the-test-images">Processing all the Test Images</h3>
<p>There are 200 test images included with the dataset between the cat and dog classes, or you can download your own pictures to try.  You can process them all like this:</p>
<pre><code class="language-bash">mkdir $DATASET/test_output_cat $DATASET/test_output_dog

imagenet --model=$NET/resnet18.onnx --input_blob=input_0 --output_blob=output_0 --labels=$DATASET/../labels.txt \
           $DATASET/test/cat $DATASET/test_output_cat

imagenet --model=$NET/resnet18.onnx --input_blob=input_0 --output_blob=output_0 --labels=$DATASET/../labels.txt \
           $DATASET/test/dog $DATASET/test_output_dog
</code></pre>
<p>In this instance, all the images will be read from the dataset's <code>test/</code> directory, and saved to the <code>test_output/</code> directory.  </p>
<p>For more info about loading/saving sequences of images, see the <a href="../Week1/setup_Jetson-Nano/#camera-setup">Camera Streaming and Multimedia</a>.</p>
<p>Next, we'll try running our re-trained model on a live camera feed.</p>
<h2 id="running-the-live-camera-program">Running the Live Camera Program</h2>
<p>If you have a furry friend at home, you can run the camera program and see how it works!  Like the previous step, <code>imagenet</code> supports extended command-line parameters for loading customized models:</p>
<pre><code class="language-bash"># C++ (MIPI CSI)
imagenet --model=$NET/resnet18.onnx --input_blob=input_0 --output_blob=output_0 --labels=$DATASET/labels.txt csi://0

# Python (MIPI CSI)
imagenet.py --model=$NET/resnet18.onnx --input_blob=input_0 --output_blob=output_0 --labels=$DATASET/labels.txt csi://0
</code></pre>
<blockquote>
<p><strong>note:</strong> for information about supported video streams and protocols, please see the <a href="aux-streaming.md">Camera Streaming and Multimedia</a> page.</p>
</blockquote>
<p><img src="https://github.com/dusty-nv/jetson-inference/raw/python/docs/images/pytorch-otto.jpg" width="500"></p>
<h2 id="creating-your-own-classification-datasets">Creating your own Classification Datasets</h2>
<p>In order to collect your own datasets for training customized models to classify objects or scenes of your choosing, we've created an easy-to-use tool called <code>camera-capture</code> (NOTE: /jetson-inference/build/aarch64/bin/camera-capture) for capturing and labeling images on your Jetson from live video:</p>
<p><img src="https://github.com/dusty-nv/jetson-inference/raw/python/docs/images/pytorch-collection.jpg" ></p>
<p>The tool will create datasets with the following directory structure on disk:</p>
<pre><code>‣ train/
    • class-A/
    • class-B/
    • ...
‣ val/
    • class-A/
    • class-B/
    • ...
‣ test/
    • class-A/
    • class-B/
    • ...
</code></pre>
<p>where <code>class-A</code>, <code>class-B</code>, ect. will be subdirectories containing the data for each object class that you've defined in a class label file.  The names of these class subdirectories will match the class label names that we'll create below.  These subdirectories will automatically be populated by the tool for the <code>train</code>, <code>val</code>, and <code>test</code> sets from the classes listed in the label file, and a sequence of JPEG images will be saved under each.</p>
<p>Note that above is the organization structure expected by the PyTorch training script that we've been using.  If you inspect the Cat/Dog and PlantCLEF datasets, they're also organized in the same way.</p>
<h2 id="creating-the-label-file">Creating the Label File</h2>
<p>Under <code>jetson-inference/python/training/classification/data</code>, create an empty directory for storing your dataset and a text file that will define the class labels (usually called <code>labels.txt</code>).  The label file contains one class label per line, and is alphabetized (this is important so the ordering of the classes in the label file matches the ordering of the corresponding subdirectories on disk).</p>
<p>Here's an example <code>labels.txt</code> file with 3 classes:</p>
<pre><code class="language-bash">class-A
class-B
class-C
</code></pre>
<p>And here's the corresponding directory structure that the tool will create:</p>
<pre><code class="language-bash">‣ train/
    • class-A/
    • class-B/
    • class-C/
‣ val/
    • class-A/
    • class-B/
    • class-C/
‣ test/
    • class-A/
    • class-B/
    • class-C/
</code></pre>
<h2 id="assignments">Assignments</h2>
<ul>
<li>If you have finished all the steps mentioned before, you are ready to perform the task.</li>
<li>Taking into account the global situation with the pandemic and COVID-19, create an IoT system that detects if a person is wearing a medical mask using the CSI Camera</li>
</ul>
<div class="admonition danger">
<p class="admonition-title">Assignment 1</p>
<p>Create your own dataset with the <strong>camera-capture</strong> tool, for example a database with three objects: (1) No person, (2) Yourself, (3) Yourself with a medical mask worn.</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Assignment 2</p>
<p>Train the ResNet-18 Model with your own dataset. Note that apart from training, you should converting the Model to ONNX</p>
</div>
<div class="admonition danger">
<p class="admonition-title">Assignment 3</p>
<p>Invoke the inference with your model create by yourself. The IoT system should detect if a person is wearing a medical mask using an image classifier based on ResNet-18 using the CSI camera</p>
</div></div>
        </div>

        <footer class="col-md-12">
            <hr>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../../../..",
                shortcuts = {"help": 191, "next": 78, "previous": 80, "search": 83};
        </script>
        <script src="../../../../js/base.js" defer></script>
        <script src="../../../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
